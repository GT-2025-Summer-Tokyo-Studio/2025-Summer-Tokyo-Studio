# -*- coding: utf-8 -*-
"""Flow_Analysis_V3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ACo80OYjBqe1iYn6zgj0shVAIz764d4K
"""

import pandas as pd
import geopandas as gpd
import osmnx as ox
import networkx as nx
from shapely.geometry import Point, LineString, Polygon
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from matplotlib import colormaps
import datetime

nihonbashi_boundary = gpd.read_file("Nihonbashi_Line.shp")
nihonbashi_boundary.set_crs(epsg=2451, inplace=True)
nihonbashi_boundary = nihonbashi_boundary.to_crs(epsg=4326)
line = nihonbashi_boundary.geometry.iloc[0]
if isinstance(line, LineString):
    if not line.is_ring:
        coords = list(line.coords)
        if coords[0] != coords[-1]:
            coords.append(coords[0])
        line = LineString(coords)
    nihonbashi_polygon = Polygon(line)
else:
    nihonbashi_polygon = line

csv_path = "05-22-GPS.csv"
required_cols = ["tripid", "recordedat", "lat", "lon", "transportmode"]
chunk_size = 10000

filtered_chunks = []

chunk_iter = pd.read_csv(csv_path, chunksize=chunk_size)
df_list = []
for chunk in tqdm(chunk_iter, desc="Filtering GPS by Nihonbashi polygon"):
    chunk = chunk.dropna(subset=['lat', 'lon', 'tripid', 'recordedat', 'transportmode'])
    chunk['lat'] = pd.to_numeric(chunk['lat'], errors='coerce')
    chunk['lon'] = pd.to_numeric(chunk['lon'], errors='coerce')
    chunk = chunk.dropna(subset=['lat', 'lon'])
    gdf_chunk = gpd.GeoDataFrame(
        chunk,
        geometry=gpd.points_from_xy(chunk['lon'], chunk['lat']),
        crs="EPSG:4326"
    )

    gdf_chunk = gdf_chunk[gdf_chunk.within(nihonbashi_polygon)]

    filtered_chunks.append(gdf_chunk)

hf_data = pd.concat(filtered_chunks, ignore_index=True)
print(f"Filtered GPS points inside Nihonbashi: {len(hf_data)}")

hf_data = hf_data[hf_data["transportmode"] == "in_vehicle"]
hf_data.loc[:, "lat"] = pd.to_numeric(hf_data["lat"], errors="coerce")
hf_data.loc[:, "lon"] = pd.to_numeric(hf_data["lon"], errors="coerce")
hf_data = hf_data.dropna(subset=["lat", "lon"])
hf_data = hf_data[(hf_data["lat"].between(-90, 90)) & (hf_data["lon"].between(-180, 180))]
hf_data["recordedat"] = pd.to_datetime(hf_data["recordedat"], format="ISO8601", utc=True)
hf_data = hf_data.sort_values("recordedat")

hf_data["geometry"] = hf_data.apply(lambda row: Point(row["lon"], row["lat"]), axis=1)
gdf = gpd.GeoDataFrame(hf_data, geometry="geometry", crs="EPSG:4326")

gdf = gdf[gdf.geometry.within(nihonbashi_polygon)].copy()
print(f"Points within Nihonbashi: {len(gdf)}")

G = ox.graph.graph_from_polygon(nihonbashi_polygon, network_type="walk")

gdf = gdf.to_crs(G.graph['crs'])
gdf["nearest_node"] = ox.distance.nearest_nodes(G, X=gdf.geometry.x, Y=gdf.geometry.y)
gdf["timestamp"] = gdf["recordedat"].dt.floor("1s")
grouped = gdf.groupby(["timestamp", "nearest_node"]).size().reset_index(name="count")

max_count = grouped["count"].max()
norm = mcolors.Normalize(vmin=1, vmax=max_count)
cmap = cm.get_cmap("Blues_r")

grouped["color"] = grouped["count"].apply(lambda x: mcolors.to_hex(cmap(norm(x))))
grouped["size"] = grouped["count"].apply(lambda x: 10 + 20 * (x / max_count))

nodes = ox.graph_to_gdfs(G, nodes=True, edges=False)
nodes = nodes.reset_index()[["osmid", "x", "y"]]
nodes.columns = ["node", "lon", "lat"]
merged = pd.merge(grouped, nodes, left_on="nearest_node", right_on="node")

merged["timestamp_str"] = merged["timestamp"].dt.strftime("%Y-%m-%dT%H:%M:%S")
merged.to_csv("05-22-driving.csv", index=False)